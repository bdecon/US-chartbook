{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6ae962",
   "metadata": {},
   "source": [
    "### Handle Charts Driven by Multiple Sources\n",
    "\n",
    "This notebook contains the codeblocks that pull from multiple sources. For example, if a chart uses both CES and CPS data, it would be run twice if located in either of those notebooks. \n",
    "\n",
    "Run this notebook after running other jobs day notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac37884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.486311Z",
     "start_time": "2023-11-11T01:21:01.513424Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import uschartbook.config\n",
    "\n",
    "from uschartbook.config import *\n",
    "from uschartbook.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1f4b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.497453Z",
     "start_time": "2023-11-11T01:21:02.488354Z"
    }
   },
   "outputs": [],
   "source": [
    "ref = lambda x: '\\\\bibitem{{{name}}} {author}. \\\\textit{{{title}}}. Available at: \\\\url{{{url}}}.'.format(**x)\n",
    "df = pd.read_csv('/home/brian/Documents/uschartbook/chartbook/references.csv')\n",
    "df = df.assign(REF = df.apply(ref, 1)).sort_values(['author', 'title']).REF\n",
    "group_size = 15\n",
    "for i in range(0, len(df), group_size):\n",
    "    group = '\\n'.join(df.iloc[i:i+group_size])\n",
    "    fname = f'reference_group_{(i/group_size)+1:.0f}.txt'\n",
    "    write_txt(text_dir / fname, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43003978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21db620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2af475e",
   "metadata": {},
   "source": [
    "### Wage Growth Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1c5b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.573943Z",
     "start_time": "2023-11-11T01:21:02.498732Z"
    }
   },
   "outputs": [],
   "source": [
    "# Average Hourly Earnings\n",
    "s = {'CES0500000003': 'AHE', 'CES0500000008': 'PNS', \n",
    "     'CES0600000008': 'Goods', 'CES0800000008': 'Serv'}\n",
    "# AHE series\n",
    "df = pd.read_csv(data_dir / 'ces_raw.csv', index_col='date', \n",
    "                 parse_dates=True)[s.keys()].rename(s, axis=1)\n",
    "dfg = df['Goods']\n",
    "ch = df.pct_change(12) * 100\n",
    "\n",
    "# Median wage from CPS\n",
    "ch['UWE_P50'] = pd.read_csv(data_dir / 'uwe_cps.csv', index_col='date', \n",
    "                            parse_dates=True)['p50_gr']\n",
    "# Wage Growth Tracker\n",
    "ch['WGT'] = pd.read_csv(data_dir / 'atl_wgt.csv', \n",
    "                        index_col='date', parse_dates=True)['bd_cps']\n",
    "\n",
    "# NIPA Wages and Salaries / NFP\n",
    "df = pd.read_csv(data_dir / 'pi_raw.csv', index_col='date', \n",
    "                 parse_dates=True)\n",
    "# Nonfarm payrolls\n",
    "nfp = pd.read_csv(data_dir / 'ces_raw.csv', index_col='date', \n",
    "                 parse_dates=True)['CES0000000001']\n",
    "ch['NIPA'] = (df['A034RC'] / nfp).dropna().pct_change(12) * 100\n",
    "\n",
    "# 3 Month Moving average for volatile series\n",
    "for i in ['UWE_P50', 'WGT', 'NIPA']:\n",
    "    ch[f'{i}_3m'] = ch[i].rolling(3).mean()\n",
    "    \n",
    "ch.loc['1989':].to_csv(data_dir / 'wages_yy_monthly.csv', \n",
    "                      index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5871633",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.587892Z",
     "start_time": "2023-11-11T01:21:02.576238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename for table\n",
    "d = {'AHE': 'Average Hourly Earnings (AHE), Private',\n",
    "     'PNS': '\\hspace{2mm} Production \\& Nonsupervisory',\n",
    "     'Goods': '\\hspace{4mm} Goods-Producing Industries',\n",
    "     'Serv': '\\hspace{4mm} Service-Providing Industries',\n",
    "     'UWE_P50': 'Usual Weekly Earnings, Median',\n",
    "     'UWE_P50_3m': 'Usual Weekly Earnings, Median (3M Avg)',\n",
    "     'WGT': 'Wage Growth Tracker, Median',\n",
    "     'WGT_3m': 'Wage Growth Tracker, Median (3M Avg)',\n",
    "     'NIPA': 'Wages \\& Salaries, Average (NIPA)',\n",
    "     'NIPA_3m': 'Wages \\& Salaries, Average (3M Avg)'}\n",
    "\n",
    "dfm = pd.read_csv(data_dir / 'wages_yy_monthly.csv', index_col='date', \n",
    "                 parse_dates=True)[d.keys()].rename(d, axis=1)\n",
    "\n",
    "# Latest 6 months\n",
    "res = dfm.iloc[-6:].iloc[::-1].T\n",
    "# Same month, prior two years\n",
    "for i in [-13, -25]:\n",
    "    res[dfm.index[i]] = dfm.iloc[i]\n",
    "    \n",
    "res.columns = [dtxt(dt)['mon6'] for dt in res.columns]\n",
    "res = res.applymap('{:.1f}'.format).replace('nan', '--')\n",
    "(res.to_csv(data_dir / 'wages_yy_monthly.tex', sep='&', lineterminator='\\\\\\ ', \n",
    "           quotechar=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cf6851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.616255Z",
     "start_time": "2023-11-11T01:21:02.589085Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Quarterly measures of wages \n",
    "# Median wage from BLS \n",
    "d = {'IndexWS': 'Wages \\& Salaries (ECI)',\n",
    "     'P50_tot': 'Usual Weekly Earnings, Median',\n",
    "     'nfbus_ulc': 'Unit Labor Cost'}\n",
    "df = (pd.read_csv(data_dir / 'uwe_main.csv', index_col='date', \n",
    "                            parse_dates=True)) \n",
    "\n",
    "# Wages and Salaries from Employment Cost Index\n",
    "eci = pd.read_csv(data_dir / 'eci.csv', index_col='date', \n",
    "                 parse_dates=True)[['IndexWS','IndexWSGoods']]\n",
    "\n",
    "# Unit Labor Costs\n",
    "ulc = pd.read_csv(data_dir / 'lprod.csv', index_col='date', \n",
    "                 parse_dates=True)[['business_ulc', 'manuf_ulc', 'nfbus_ulc']]\n",
    "\n",
    "df = pd.concat([df, eci, ulc], axis=1)\n",
    "\n",
    "# Average Hourly Earnings PNS, Goods\n",
    "df['AHEGoods'] = dfg.resample('QS').mean()\n",
    "\n",
    "dfy = df.pct_change(4, fill_method=None) * 100 \n",
    "dfy.loc['1989':].to_csv(data_dir / 'wages_yy_quarterly.csv', \n",
    "                      index_label='date')\n",
    "dfq = df.pct_change(fill_method=None) * 400 \n",
    "\n",
    "# Gender Wage Gap\n",
    "gwg = (df['P50_women'] / df['P50_men']).rolling(4).mean()\n",
    "gwg.name = 'GWG'\n",
    "gwg.loc['1989':].multiply(100).to_csv(data_dir / 'gwg.csv', \n",
    "                      index_label='date')\n",
    "node = end_node(gwg * 100, 'red', date='qs', offset=0.1,\n",
    "                anchor='south', align='center', colon=False)\n",
    "write_txt(text_dir / 'gwg_node.txt', node)\n",
    "\n",
    "# Sumary Table\n",
    "# Latest 5 quarters\n",
    "res = dfy.iloc[-5:].iloc[::-1].T\n",
    "# Same quarter, prior two years\n",
    "for i in [-9, -13, -17]:\n",
    "    res[dfy.index[i]] = dfy.iloc[i]\n",
    "    \n",
    "res = res.loc[d.keys()].rename(d)\n",
    "res.columns = [dtxt(dt)['qtr4'] for dt in res.columns]\n",
    "res = res.applymap('{:.1f}'.format).replace('nan', '--')\n",
    "(res.to_csv(data_dir / 'wages_yy_quarterly.tex', sep='&', lineterminator='\\\\\\ ', \n",
    "           quotechar=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f052f861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.623163Z",
     "start_time": "2023-11-11T01:21:02.617713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1989, the gender wage gap was 30 percent; women were paid 70 cents for each dollar men were paid. From 1989 to 2006, the gap closed at a rate of 0.74 percentage point per year. From 2006 to 2019 Q4, the gap closed at a rate of only 0.03 percentage point per year.\n",
      "\n",
      "Over the year ending 2023 Q3, the gender wage gap is 16.4 percent; women are paid 83.6 cents on the dollar. Pre-pandemic, in 2019 Q4, the gap was 18.4 percent.\n"
     ]
    }
   ],
   "source": [
    "# Gender Wage Gap Text\n",
    "ltdt = dtxt(gwg.index[-1])['qtr1']\n",
    "gap = gwg.iloc[-1] * 100\n",
    "gap2 = 100 - gap\n",
    "cmp = '2019-10-01'\n",
    "cmpdt = dtxt(cmp)['qtr1']\n",
    "gapcmp = (1.0 - gwg.loc['2019-10-01']) * 100\n",
    "dt1 = '1989-01-01'\n",
    "dt2 = '2006-01-01'\n",
    "dt3 = '2006-01-01'\n",
    "dt4 = cmp\n",
    "\n",
    "# Annualize rates (divide by 59 quarters)\n",
    "hgap1 = value_text((gwg.loc[dt2] - gwg.loc[dt1]) * (100 / (59 / 4)), \n",
    "                   'plain', 'pp', digits=2)\n",
    "hgap2 = value_text((gwg.loc[dt4] - gwg.loc[dt3]) * (100 / (59 / 4)), \n",
    "                   'plain', 'pp', digits=2)\n",
    "\n",
    "text = ('In 1989, the gender wage gap was 30 percent; women were paid '+\n",
    "        '70 cents for each dollar men were paid. From 1989 to 2006, '+\n",
    "        f'the gap closed at a rate of {hgap1} per year. '+\n",
    "        'From 2006 to 2019 Q4, the gap closed at a '+\n",
    "        f'rate of only {hgap2} per year.\\n\\n'+\n",
    "        f'Over the year ending {ltdt}, the gender wage gap is {gap2:.1f} '+\n",
    "        f'percent; women are paid {gap:.1f} cents on the dollar. '+\n",
    "        f'Pre-pandemic, in 2019 Q4, the gap was {gapcmp:.1f} percent.')\n",
    "write_txt(text_dir / 'gwg.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b59ee922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.645028Z",
     "start_time": "2023-11-11T01:21:02.625379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nearly all measures show one-year nominal wage growth that is falling but still above the pre-pandemic rate.\n"
     ]
    }
   ],
   "source": [
    "# Summary text \n",
    "d, d2 = {}, {} # One-year change and change since 2019\n",
    "# Monthly series\n",
    "for s in ['AHE', 'PNS', 'Goods', 'WGT', 'WGT_3m']:\n",
    "    d[s] = ch[s].dropna().diff(12).iloc[-1]\n",
    "    d2[s] = ch[s].dropna().iloc[-1] - ch.loc['2019', s].mean()\n",
    "    \n",
    "# Quarterly Series    \n",
    "for s in ['IndexWS', 'IndexWSGoods', 'p50uwe']:\n",
    "    d[s] = dfy[s].dropna().diff(4).iloc[-1]\n",
    "    d2[s] = dfy[s].dropna().iloc[-1] - dfy.loc['2019', s].mean()\n",
    "    \n",
    "chlt = pd.DataFrame({'One-year': d, 'Since 2019': d2})\n",
    "keych = chlt.loc[['IndexWS', 'WGT_3m']].mean()\n",
    "chlt.loc['keych'] = keych\n",
    "chdf = pd.concat([pd.cut(chlt[c], [-50, -0.49, 0.49, 50], \n",
    "                         labels=['below', 'same', 'above']) \n",
    "           for c in chlt.columns], axis=1)\n",
    "\n",
    "ref = chdf.drop('keych')\n",
    "keym = chdf.loc['keych']\n",
    "\n",
    "# What percent of the series match the key series?\n",
    "pct = pd.DataFrame(map(lambda k: ref[k]==keym[k], ref)).all().mean()\n",
    "\n",
    "keydir1 = ('falling' if keych['One-year'] <= -0.49 else 'rising' \n",
    "           if keych['One-year'] >=0.49 else 'stable')\n",
    "keydir2 = ('below' if keych['Since 2019'] <= -0.49 else 'above' \n",
    "           if keych['Since 2019'] >=0.49 else 'in line with')\n",
    "\n",
    "keyt = 'key'\n",
    "if pct > 0.8:\n",
    "    keyt = 'nearly all'\n",
    "elif pct > 0.5:\n",
    "    keyt = 'most'\n",
    "    \n",
    "but = 'but still' if ((keydir1 == 'falling') & (keydir2 == 'above') | \n",
    "                (keydir1 == 'rising') & (keydir2 == 'below')) else 'and'    \n",
    "\n",
    "text = (f'{keyt} measures show one-year nominal wage growth that is {keydir1} '+\n",
    "        f'{but} {keydir2} the pre-pandemic rate.')\n",
    "write_txt(text_dir / 'wage_rec_summary.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c512bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a12ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12bc512b",
   "metadata": {},
   "source": [
    "### Recessions Table / Sahm Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd8b602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.653577Z",
     "start_time": "2023-11-11T01:21:02.646181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve recession info from FRED\n",
    "#rec = fred_df('USREC')\n",
    "#rec.to_csv(data_dir / 'recessions_raw.csv', index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6c2eff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.754783Z",
     "start_time": "2023-11-11T01:21:02.657367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the early 1990s recession, output contracted for eight months and unemployment was higher than its pre-recession average for 63 months. The drop in output was smaller during the early 2000s recession, but unemployment rates took almost 16 years to recover.\n",
      "\n",
      "The 2008--2009 great recession, caused by the collapse of a housing bubble, was very severe. The recession lasted 18 months, with higher rates of unemployment lasting 89 months. The most-recent COVID-19 recession was extremely severe and also extremely short-lived, lasting only two months, but with output reduced 9.1 percent.\n"
     ]
    }
   ],
   "source": [
    "rec = pd.read_csv(data_dir / 'recessions_raw.csv', \n",
    "                  index_col='date', parse_dates=True)\n",
    "first = rec[(rec.VALUE==1) & (rec.VALUE.shift(1) == 0)]\n",
    "post = rec[(rec.VALUE==0) & (rec.VALUE.shift(1) == 1)]\n",
    "names = [' \\ Early `90s Recession', ' \\ Early `00s Recession', \n",
    "         ' \\ Great Recession', ' \\ COVID-19 Recession']\n",
    "recs = (pd.Series(data=first.index, index=names)\n",
    "        .rename('First').to_frame())\n",
    "recs['Last'] = rec[(rec.VALUE==1) & (rec.VALUE.shift(-1) == 0)].index\n",
    "recs['Pre'] = rec[(rec.VALUE==0) & (rec.VALUE.shift(-1) == 1)].index\n",
    "recs['Post'] = post.index\n",
    "dur = [i.n for i in (post.index.to_period('M') - \n",
    "                     first.index.to_period('M'))]\n",
    "recs['Dur'] = pd.Series(data=dur, index=recs.index)\n",
    "recs['PrevEnd'] = recs['Post'].shift(1)\n",
    "recs.loc[' \\ Early `90s Recession', 'PrevEnd'] = pd.to_datetime('1989-01-01')\n",
    "recs['NextStart'] = recs['Pre'].shift(-1)\n",
    "recs.loc[' \\ COVID-19 Recession', 'NextStart'] = cps_date()\n",
    "recs['Start'] = recs.First.apply(lambda x: dtxt(x)['mon2'])\n",
    "recs['End'] = recs.Last.apply(lambda x: dtxt(x)['mon2'])\n",
    "rgdp = nipa_df(retrieve_table('T10106')['Data'], ['A191RX'])['A191RX']\n",
    "unrate = pd.read_csv(data_dir / 'jobs_report_main.csv', index_col='date', \n",
    "                 parse_dates=True)['Total']\n",
    "for row in recs.itertuples():\n",
    "    # Real GDP change\n",
    "    vprev = rgdp.loc[:row.Pre].max()\n",
    "    vmin = rgdp.loc[row.First:row.NextStart].min()\n",
    "    ch = ((vmin / vprev) - 1) * 100\n",
    "    recs.loc[row.Index, 'GDPch'] = ch\n",
    "    # Unemployment rate change and duration\n",
    "    pravg = unrate.loc[row.Pre - pd.DateOffset(years=3): row.Pre].mean()\n",
    "    vmax = unrate.loc[row.First:row.NextStart].max()\n",
    "    uch = vmax - pravg\n",
    "    rdt = (unrate.loc[row.Last:].loc[(unrate <= pravg)].index[0] \n",
    "           if pravg >= unrate.iloc[-1] else '--')\n",
    "    rtime = (int((rdt.to_period('M') - row.Last.to_period('M')).n) \n",
    "             if rdt != '--' else '--')\n",
    "    recs.loc[row.Index, 'Unratech'] = uch    \n",
    "    recs.loc[row.Index, 'RecoDate'] = rdt\n",
    "    recs.loc[row.Index, 'RecoTime'] = str(rtime)\n",
    "recs['GDPcht'] =  recs.GDPch.apply('{:.1f}'.format)\n",
    "recs['Uncht'] =  recs.Unratech.apply('+{:.1f}'.format)\n",
    "tbl = recs[['Start', 'End', 'Dur', 'GDPcht', 'Uncht', 'RecoTime']]\n",
    "tbl.columns = ['Start \\ \\ \\ Month', 'End \\ \\ \\ \\ \\ \\ Month', \n",
    "               'Recession Duration, Months', \n",
    "               'GDP Percent Change', 'Unemp. Rate Change*', \n",
    "               'Unemp. Rate Recovery, Months**']\n",
    "tbl.to_csv(data_dir / 'recession.tex', sep='&', \n",
    "           lineterminator='\\\\\\ ', quotechar=' ')\n",
    "\n",
    "un3 = unrate.rolling(3).mean()\n",
    "sahm = (un3 - un3.rolling(12).min()).dropna()\n",
    "sahm.to_csv(data_dir / 'sahm.csv', index_label='date', \n",
    "            header=True)\n",
    "\n",
    "# End Node\n",
    "node = end_node(sahm, 'blue!60!black', digits=2, date='m', offset=0.35)\n",
    "write_txt(text_dir / 'sahm_node.txt', node)\n",
    "\n",
    "bar = pd.Series(index=[sahm.index[0], sahm.index[-1]], \n",
    "                data=[0.5, 0.5], name='Bar')\n",
    "bar.to_csv(data_dir / 'sahm_bar.csv', index_label='date', \n",
    "           header=True)\n",
    "node = end_node(bar, 'gray', loc='start')\n",
    "write_txt(text_dir / 'sahm_bar_node.txt', node)\n",
    "marks = (sahm.loc[(sahm > 0.5) & (sahm.shift(1) < 0.5)]\n",
    "             .rename('Mark').to_frame())\n",
    "marks['Intersect'] = len(marks) * [0.5]\n",
    "marks.to_csv(data_dir / 'sahm_marks.csv', index_label='date')\n",
    "\n",
    "dur90 = numbers[f'{recs.Dur.iloc[0]:.1f}']\n",
    "unrec90 = recs.RecoTime.iloc[0]\n",
    "unrec00 = round(int(recs.RecoTime.iloc[1]) / 12)\n",
    "durgr = recs.Dur.iloc[2]\n",
    "unrecgr = recs.RecoTime.iloc[2]\n",
    "durco = numbers[f'{recs.Dur.iloc[3]:.1f}']\n",
    "gdpco = abs(recs.GDPch.iloc[3])\n",
    "\n",
    "text = ('During the early 1990s recession, output contracted '+\n",
    "        f'for {dur90} months and unemployment was higher '+\n",
    "        f'than its pre-recession average for {unrec90} months. '+\n",
    "        'The drop in output was smaller during the '+\n",
    "        'early 2000s recession, but unemployment rates '+\n",
    "        f'took almost {unrec00} years to recover.\\n\\n'+\n",
    "        'The 2008--2009 great recession, caused by the '+\n",
    "        'collapse of a housing bubble, was very severe. '+\n",
    "        f'The recession lasted {durgr} months, with higher '+\n",
    "        f'rates of unemployment lasting {unrecgr} months. The '+\n",
    "        'most-recent COVID-19 recession was extremely severe '+\n",
    "        f'and also extremely short-lived, lasting only {durco} '+\n",
    "        f'months, but with output reduced {gdpco:.1f} percent.')\n",
    "write_txt(text_dir / 'recessions.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a7d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccace1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b02ca191",
   "metadata": {},
   "source": [
    "### Gross Labor Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c8631d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.805018Z",
     "start_time": "2023-11-11T01:21:02.757271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increased at an average annualized rate of 5.6 percent over the year ending 2023 Q3. Changes in wages contributed three percentage points, and changes in total hours worked contributed 2.5 percentage points.\n"
     ]
    }
   ],
   "source": [
    "df = (pd.read_csv(data_dir / 'jobs_report_main2.csv', parse_dates=[0])\n",
    "         .set_index('date')[['avghrstot', 'EMPsa']])\n",
    "emp = (df['avghrstot'] * df['EMPsa']).rename('Total')\n",
    "coe = nipa_df(retrieve_table('T20100')['Data'], ['A033RC'])\n",
    "data = coe.join(emp.resample('QS').mean()).dropna()\n",
    "data['coe_inp'] = data['A033RC'] / data['Total']\n",
    "data['wage'] = data['coe_inp'] * data['Total'].iloc[0]\n",
    "data['work'] = data['A033RC'] - data['wage']\n",
    "\n",
    "# Calculate contributions to growth\n",
    "result = (growth_contrib(data, 'A033RC')[['work', 'wage']]\n",
    "          .rolling(4).mean().dropna())\n",
    "result['sum'] = result.sum(axis=1)\n",
    "result.to_csv(data_dir / 'gli.csv', index_label='date')\n",
    "\n",
    "# Horizontal bar at 5\n",
    "start = dtxt(result.index[0] - pd.DateOffset(months=1))['datetime']\n",
    "end = dtxt(result.index[-1] + pd.DateOffset(months=3))['datetime']\n",
    "hbar = (f'\\draw [dotted, thick] (axis cs:{{{start}}}, 5) -- '+\n",
    "        f'(axis cs:{{{end}}}, 5);')\n",
    "write_txt(text_dir / 'gli_hbar2.txt', hbar) \n",
    "\n",
    "# Text\n",
    "ltdate = dtxt(result.index[-1])['qtr1']\n",
    "totch = value_text(result['sum'].iloc[-1], adj='avg_ann', \n",
    "                   threshold=0.1)\n",
    "wage = result['wage'].iloc[-1]\n",
    "work = result['work'].iloc[-1]\n",
    "    \n",
    "txt2 = value_text(wage, 'contribution', 'pp')\n",
    "txt3 = value_text(work, 'contribution', 'pp')\n",
    "\n",
    "text = (f'{totch} over the year ending {ltdate}. Changes in wages {txt2}, '+\n",
    "        f'and changes in total hours worked {txt3}.')\n",
    "write_txt(text_dir / 'gli.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39514a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f522d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19456aef",
   "metadata": {},
   "source": [
    "### Employment rate - disability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e848461a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.812716Z",
     "start_time": "2023-11-11T01:21:02.808601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve series from FRED for comparison\n",
    "#bls = fred_df('LNU02374597')\n",
    "#bls = bls.rename({'VALUE': 'BLS'}, axis=1)\n",
    "#pd.concat([bls, data], axis=1).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ed2c76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:02.833103Z",
     "start_time": "2023-11-11T01:21:02.815459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of October 2023, BLS reports a 22.6 percent employment rate for individuals aged 16 and over with at least one disability (see {\\color{red}\\textbf{---}}). This marks a 0.6 percentage point increase over the past year, and a jump of 3.5 percentage points since October 2019.\n"
     ]
    }
   ],
   "source": [
    "# BLS data on age 16+\n",
    "df = pd.read_csv(data_dir / 'jobs_report_main2.csv', index_col='date', \n",
    "                 parse_dates=True)['empdis'].dropna().rename('BLS')\n",
    "df.to_csv(data_dir / 'dis_emp_rate_bls.csv', index_label='date', \n",
    "         header=True)\n",
    "node = end_node(df, 'red', date='m', \n",
    "                size=1.1, offset=-0.2) \n",
    "write_txt(text_dir / 'dis_emp_node_bls.txt', node)\n",
    "\n",
    "bdt = dtxt(df.index[-1])['mon1']\n",
    "prdt = dtxt(df.index[-49])['mon1']\n",
    "prdt2 = dtxt(df.index[-13])['mon1']\n",
    "ltval = df.iloc[-1]\n",
    "prch = ltval - df.iloc[-49]\n",
    "vch = value_text(prch, 'increase_of', \n",
    "                 ptype='pp', threshold=0.1)\n",
    "if prch > 3:\n",
    "    vch = vch.replace('an increase of ', 'a jump of ')\n",
    "\n",
    "vch2 = value_text(ltval - df.iloc[-13], 'increase_end', \n",
    "                  ptype='pp', threshold=0.1)\n",
    "\n",
    "text = (f'As of {bdt}, BLS reports a {ltval:.1f} percent employment '+\n",
    "        'rate for individuals aged 16 and over with at least one '+\n",
    "        f'disability {c_line(\"red\")}. This marks '+\n",
    "        f'{vch2} over the past year, and {vch} since {prdt}.')\n",
    "write_txt(text_dir / 'dis_rate_bls.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc719a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:04.447258Z",
     "start_time": "2023-11-11T01:21:02.835736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In October 2023, 33.6 million people age 16 and older report at least one such disability, of which 16.9 million are under age 65. \n",
      "For those age 16 to 64 with disabilities, the employment rate is 37.3 percent in October 2023 (see {\\color{blue}\\textbf{---}}), a one-year increase of 1.7 percentage points, and a 6.5 percentage point increase since 2019.\n",
      "\n",
      "In 2013, during the sluggish recovery from the great recession, the employment rate for those age 16 to 64 with a disability averaged 26.8 percent. \n"
     ]
    }
   ],
   "source": [
    "# CPS data on more narrow age groups\n",
    "columns = ['MONTH', 'YEAR', 'LFS', 'DISABILITY', 'BASICWGT', 'AGE', 'NILFREASON', 'FEMALE']\n",
    "raw = (pd.concat([pd.read_feather(cps_dir / f'cps{year}.ft', columns=columns)\n",
    "                 for year in range(2008, 2024)]))\n",
    "\n",
    "d = {'Age2554': '25 <= AGE <= 54 and DISABILITY == 1',\n",
    "     'Age1664': '16 <= AGE <= 64 and DISABILITY == 1',\n",
    "     'Age55plus': 'AGE >= 55 and DISABILITY == 1'}\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Employment rate for each age group\n",
    "for name, query in d.items():\n",
    "    df = raw.query(query)\n",
    "    data[name] = ((df.groupby(['YEAR', 'MONTH', 'LFS']).BASICWGT.sum() / \n",
    "                   df.groupby(['YEAR', 'MONTH']).BASICWGT.sum() * 100)\n",
    "                  .unstack()['Employed'].dropna())\n",
    "data.index = [pd.to_datetime(f'{i[0]}-{i[1]}-01') for i in data.index]\n",
    "\n",
    "# End nodes \n",
    "s = {#'Age2554': 'orange', \n",
    "     'Age1664': 'blue'}\n",
    "nodes  ='\\n'.join([end_node(data[series], color, \n",
    "                            date='m', size=1.1, offset=-0.1) \n",
    "                   for series, color in s.items()])\n",
    "write_txt(text_dir / 'dis_emp_nodes_cps.txt', nodes)\n",
    "\n",
    "data.applymap('{:.1f}'.format).to_csv(data_dir / 'dis_emp_rate_cps.csv', \n",
    "            index_label='date', header=True)\n",
    "\n",
    "# Count with disability\n",
    "ltdt = dtxt(cps_date())['mon1']\n",
    "cps_mo = cps_date().month\n",
    "cps_yr = cps_date().year\n",
    "tmp = raw.query('MONTH == @cps_mo and YEAR == @cps_yr')\n",
    "td = tmp.query('DISABILITY == 1').BASICWGT.sum() / 1_000_000\n",
    "td2 = tmp.query('DISABILITY == 1 and AGE < 65').BASICWGT.sum() / 1_000_000\n",
    "\n",
    "text = (f'In {ltdt}, {td:.1f} million people age 16 and older '+\n",
    "        f'report at least one such disability, of which {td2:.1f} '\n",
    "        f'million are under age 65. ')\n",
    "write_txt(text_dir / 'dis_rate.txt', text)\n",
    "print(text)\n",
    "\n",
    "dft = data['Age1664']\n",
    "ltval = dft.iloc[-1]\n",
    "ltdt = dtxt(dft.index[-1])['mon1']\n",
    "prdt = dtxt(dft.index[-49])['year']\n",
    "ch = dft.iloc[-1] - dft.iloc[-49]\n",
    "valch = value_text(ch, 'increase_end', ptype='pp')\n",
    "ch2 = dft.iloc[-1] - dft.iloc[-13]\n",
    "valch2 = value_text(ch2, 'increase_of', ptype='pp', time_str='one-year ')\n",
    "val13 = dft.loc['2013'].mean()\n",
    "cl = c_line('blue')\n",
    "\n",
    "text = ('For those age 16 to 64 with disabilities, the '+\n",
    "        f'employment rate is {ltval:.1f} percent in {ltdt} {cl}, '+\n",
    "        f'{valch2}, and {valch} since {prdt}.\\n\\n'+\n",
    "        'In 2013, during the sluggish recovery from the great recession, '+\n",
    "        'the employment rate for those age 16 to 64 with a disability '+\n",
    "        f'averaged {val13:.1f} percent. ')\n",
    "write_txt(text_dir / 'dis_rate_cps.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c666ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4b7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e864644f",
   "metadata": {},
   "source": [
    "### Average Weekly Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa126e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:04.997417Z",
     "start_time": "2023-11-11T01:21:04.448506Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/miniconda3/lib/python3.8/site-packages/statsmodels/tsa/x13.py:203: X13Warning: WARNING: At least one visually significant trading day peak has been\n",
      "          found in the estimated spectrum of the regARIMA residuals.\n",
      "  warn(errors, X13Warning)\n"
     ]
    }
   ],
   "source": [
    "hrs = {'CES0500000002': 'ceshrstot',\n",
    "       'CES0600000002': 'ceshrsgoods',\n",
    "       'CES0800000002': 'ceshrsserv',\n",
    "       'CES0500000007': 'ceshrspns'}\n",
    "\n",
    "df = pd.read_csv(data_dir / 'ces_raw.csv', index_col='date', \n",
    "                   parse_dates=True)\n",
    "df = df[hrs.keys()].rename(hrs, axis=1)\n",
    "\n",
    "df2 = pd.concat([df, (pd.read_csv(data_dir / 'jobs_report_main2.csv', parse_dates=['date'])\n",
    "        .set_index('date'))], axis=1)\n",
    "cps = pd.read_csv(data_dir / 'uslhrs.csv', \n",
    "                  index_col='name', parse_dates=True)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['TOTCES'] = df2['ceshrstot']\n",
    "data['TOTLFS'] = df2['avghrstot']\n",
    "data['SERVNSA'] = df2['avghrsserv']\n",
    "data['SERVSA'] = x13_arima_analysis(df2['avghrsserv'].dropna()).seasadj\n",
    "data['PNS'] = df2['ceshrspns']\n",
    "data['PTECONNSA'] = df2['avghrsptecon']\n",
    "data['PTECONSA'] = x13_arima_analysis(df2['avghrsptecon'].dropna()).seasadj\n",
    "data['TOTCPS'] = cps['Total']\n",
    "data['PA_CPSFT'] = cps['Age2554FT']\n",
    "data['FT_CPS'] = cps['FT']\n",
    "data['PA_CPSPT'] = cps['Age2554PT']\n",
    "data['PT_CPS'] = cps['PT']\n",
    "\n",
    "data.loc['1989':].to_csv(data_dir / 'hours.csv', index_label='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "828473f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:05.008935Z",
     "start_time": "2023-11-11T01:21:04.998957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve data\n",
    "data = pd.read_csv(data_dir / 'hours.csv', index_col='date', \n",
    "                   parse_dates=True)\n",
    "ltval = data['TOTLFS'].iloc[-1]\n",
    "ltdate = dtxt(data.index[-1])['mon1']\n",
    "feb20val = data.loc['2020-02-01', 'TOTLFS']\n",
    "compare = compare_text(ltval, feb20val, [0.2, 1.5, 3.0])\n",
    "avg90 = data.loc['1998':'2000', 'TOTLFS'].mean()\n",
    "gfclow = data.loc['2005': '2012', 'TOTLFS'].min()\n",
    "gfclowdt = dtxt(data.loc['2005': '2012', 'TOTLFS'].idxmin())['mon1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0be213a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:05.018157Z",
     "start_time": "2023-11-11T01:21:05.010294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual hours worked by people at work in all industries during the survey reference week average 38.4 in October 2023 (see {\\color{blue}\\textbf{---}}), slightly below the 38.8 average actual hours worked in February 2020. Average actual hours for this group average 39.6 from 1998 through 2000, and fell to a great recession low of 37.4 in February 2010.\n",
      "Those in service occupations work fewer hours on average, with 34.8 average weekly hours in October 2023, slightly below the 35.2 average in February 2020. Those part-time for economic reasons (see {\\color{red!90!black}\\textbf{---}}) work an average of 23.3 hours per week in October 2023. \n",
      "In October 2023, production and non-supervisory workers (see {\\color{orange}\\textbf{---}}), about four of every five employees, worked 33.7 hours per week on average, in line with the 33.6 average weekly hours in February 2020 and substantially below the 1998--2000 average of 34.4 hours.\n"
     ]
    }
   ],
   "source": [
    "text = ('Actual hours worked by people at work in all industries '+\n",
    "        f'during the survey reference week average {ltval:.1f} in {ltdate} '+\n",
    "        '(see {\\color{blue}\\\\textbf{---}}), '+\n",
    "        f'{compare} the {feb20val:.1f} average actual hours worked in February '+\n",
    "        f'2020. Average actual hours for this group average {avg90:.1f} from '+\n",
    "        '1998 through 2000, and fell to a great recession low of '+\n",
    "        f'{gfclow:.1f} in {gfclowdt}.')\n",
    "write_txt(text_dir / 'hours_tot.txt', text)\n",
    "print(text)\n",
    "\n",
    "ltval2 = data.SERVSA.iloc[-1]\n",
    "feb20val2 = data.loc['2020-02-01', 'SERVSA']\n",
    "compare2 = compare_text(ltval2, feb20val2, [0.2, 0.6, 2.5])\n",
    "pteval = data.PTECONSA.iloc[-1]\n",
    "text = ('Those in service occupations work '+\n",
    "        f'fewer hours on average, with {ltval2:.1f} average '+\n",
    "        f'weekly hours in {ltdate}, {compare2} the {feb20val2:.1f} '+\n",
    "        'average in February 2020. Those part-time '+\n",
    "        'for economic reasons (see {\\color{red!90!black}\\\\textbf{---}}) '+\n",
    "        f'work an average of {pteval:.1f} hours per week in {ltdate}. ')\n",
    "write_txt(text_dir / 'hours_lfs2.txt', text)\n",
    "print(text)\n",
    "\n",
    "ltval3 = data.PNS.iloc[-1]\n",
    "feb20val3 = data.loc['2020-02-01', 'PNS']\n",
    "compare3 = compare_text(ltval3, feb20val3, [0.2, 0.6, 2.5])\n",
    "val98 = data.loc['1998':'2000', 'PNS'].mean()\n",
    "compare4 = compare_text(ltval3, val98, [0.2, 0.6, 2.5])\n",
    "text = (f'In {ltdate}, '+\n",
    "        'production and non-supervisory workers (see {\\color{orange}\\\\textbf{---}})'+\n",
    "        ', about four of every five employees, '+\n",
    "        f'worked {ltval3:.1f} hours per week on average, '+\n",
    "        f'{compare3} the {feb20val3:.1f} average weekly hours in February 2020 and '+\n",
    "        f'{compare4} the 1998--2000 average of {val98:.1f} hours.')\n",
    "write_txt(text_dir / 'hours_ces.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "207ee812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:05.032174Z",
     "start_time": "2023-11-11T01:21:05.020925Z"
    }
   },
   "outputs": [],
   "source": [
    "d = {'TOTCES': 'Total Actual, CES',\n",
    "     'PNS': '\\hspace{2mm} Production \\& Non-Supervisory, CES ({\\color{orange}\\\\textbf{---}} )',\n",
    "     'TOTLFS': 'Total Actual, LFS ({\\color{blue}\\\\textbf{---}})',\n",
    "     'PTECONSA': '\\hspace{2mm} Part-Time for Economic Reasons, LFS ({\\color{red!90!black}\\\\textbf{---}})',\n",
    "     'SERVSA': '\\hspace{2mm} Services Occupations, LFS',\n",
    "     'TOTCPS': 'Total Usual, CPS',\n",
    "     'FT_CPS': '\\hspace{2mm} Full-Time, All Ages, CPS',\n",
    "     'PA_CPSFT': '\\hspace{4mm} Full-Time, Age 25 to 54, CPS',\n",
    "     'PT_CPS': '\\hspace{2mm} Part-Time, All Ages, CPS',\n",
    "     'PA_CPSPT': '\\hspace{4mm} Part-Time, Age 25 to 54, CPS'}\n",
    "\n",
    "dft = data[d.keys()].rename(d, axis=1)\n",
    "tbl = dft.iloc[[-1, -2, -3, -13]].T\n",
    "tbl.columns = [dtxt(c)['mon8'] for c in tbl.columns]\n",
    "tbl['2019'] = dft.loc['2019'].mean()\n",
    "tbl['2015'] = dft.loc['2015'].mean()\n",
    "tbl['2010'] = dft.loc['2010'].mean()\n",
    "\n",
    "tbl.round(1).to_csv(data_dir / 'hoursworked_table.tex', sep='&', \n",
    "             lineterminator='\\\\\\ ', quotechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba203f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b1f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07f39597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:39:43.731164Z",
     "start_time": "2023-11-10T12:39:43.728241Z"
    }
   },
   "source": [
    "### Pay - Productivity Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ad32559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:05.196572Z",
     "start_time": "2023-11-11T01:21:05.033720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gather data\n",
    "# Productivity\n",
    "ndp = nipa_df(retrieve_table('T11706')['Data'], ['A362RX'])['A362RX']\n",
    "hrs = pd.read_csv(data_dir / 'gdpjobslvl.csv', \n",
    "                   index_col='date', parse_dates=True)['TOT_HRS']\n",
    "lprod = (ndp / hrs).rolling(4).mean().loc['1989':].dropna()\n",
    "nprod = lprod / lprod.iloc[0]\n",
    "nprod.name = 'Productivity'\n",
    "\n",
    "# Wages - ECI\n",
    "df = pd.read_csv(data_dir / 'eci.csv', index_col='date', \n",
    "                 parse_dates=True)[['IndexWS_All_SA_SIC', 'IndexWS']]\n",
    "df0 = df.loc[:'2000-10-01', 'IndexWS_All_SA_SIC']\n",
    "df1 = df.dropna().mean(axis=1) # Average of SIC and NAICs during overlap\n",
    "df2 = df.loc['2006-01-01':, 'IndexWS']\n",
    "res = pd.concat([df0, df1, df2])\n",
    "\n",
    "defl = nipa_df(retrieve_table('T20304')['Data'], ['DPCERG'])['DPCERG']\n",
    "rw = (res / defl)\n",
    "rw = rw.rolling(4).mean().dropna().loc['1989':]\n",
    "eci = rw / rw.loc['1989-10-01']\n",
    "eci.name = 'Average'\n",
    "\n",
    "# Wages - Median\n",
    "nw = pd.read_csv(data_dir / 'uwe_cps.csv', index_col='date', \n",
    "                 parse_dates=True)['p50']\n",
    "cpi = pd.read_csv(data_dir / 'cpi_raw.csv', \n",
    "                 index_col='date', parse_dates=True)['All items']\n",
    "rw = x13_arima_analysis((nw / cpi).dropna().resample('QS').mean()).seasadj\n",
    "rw = rw.rolling(4).mean().dropna()\n",
    "rmw = rw / rw.iloc[0]\n",
    "rmw.name = 'Median'\n",
    "\n",
    "# Combine data\n",
    "res = pd.concat([rmw, eci, nprod], axis=1) * 100\n",
    "res.loc['1989-10-01':].to_csv(data_dir / 'payprod.csv', index_label='date')\n",
    "resa = cagr(res.loc['1989-10-01':]) # Annual growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ab69fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:05.232340Z",
     "start_time": "2023-11-11T01:21:05.198162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since 1989, annualized net output growth is 2.3 percent, net productivity growth is 1.3 percent, and population growth is 0.9 percent.\n",
      "\n",
      "While the US has modest labor productivity growth over the past few decades, wages have not kept pace. The average wage has grown by 0.7 percent per year since 1989, and the median wage has increased by 0.4 percent per year. \n"
     ]
    }
   ],
   "source": [
    "# Text\n",
    "ndpgr = value_text(cagr(ndp.loc['1989-10-01':]), 'plain')\n",
    "prodgr = value_text(cagr(nprod.loc['1989-10-01':]), 'plain')\n",
    "pop = nipa_df(retrieve_table('T20100')['Data'], ['B230RC'])['B230RC']\n",
    "popgr = value_text(cagr(pop.loc['1989-10-01':]), 'plain')\n",
    "wmdgr = value_text(cagr(res.Median.loc['1989-10-01':]), 'increase_by')\n",
    "wmngr = value_text(cagr(res.Average.loc['1989-10-01':]), 'increase_by', \n",
    "                   casual=True).replace('grew', 'grown')\n",
    "\n",
    "text = (f'Since 1989, annualized net output growth is {ndpgr}, net '+\n",
    "        f'productivity growth is {prodgr}, and population growth is {popgr}.\\n\\n'+\n",
    "        f'While the US has modest labor productivity growth over the past few '+\n",
    "        'decades, wages have not kept pace. The average wage has '+\n",
    "        f'{wmngr} per year since 1989, and the median wage has '+\n",
    "        f'{wmdgr} per year. ')\n",
    "write_txt(text_dir / 'payprod.txt', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "067dce3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T01:21:05.238901Z",
     "start_time": "2023-11-11T01:21:05.233591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Settings for plot\n",
    "cmax, cmin = resa.max(), resa.min()\n",
    "thresh = (cmax - cmin) * 0.5 #Bigger bars labeled inside\n",
    "\n",
    "v = {'Median': ['\\\\footnotesize Median (CPS)', 'green!90!blue', 'black', 0], \n",
    "    'Average': ['\\\\footnotesize Average (ECI)', 'blue!80!black', 'white', 1],\n",
    "    'Productivity': ['\\\\footnotesize Net Labor \\\\ \\\\footnotesize Productivity', \n",
    "                     'cyan!90!white', 'black', 2.4]}\n",
    "txt = []\n",
    "for k, [name, color, tcolor, y] in v.items():\n",
    "    x = resa[k].round(3)\n",
    "    bar = f'\\\\addplot[{color}] coordinates {{({x}, {y})}};'\n",
    "    vtc = 'black'\n",
    "    tx = f'{resa[k]:.1f}\\%'\n",
    "    if abs(x) > thresh:  # Some value labels inside of bars\n",
    "        vt = f'\\scriptsize \\color{{{tcolor}}} \\\\textbf{{{tx}}}'\n",
    "        inside = True\n",
    "    else:\n",
    "        vt = f'\\scriptsize {tx}'\n",
    "        inside = False\n",
    "    if x > 0:\n",
    "        ytlab = 'left, align=right'\n",
    "        vtlab = 'left, align=right' if inside == True else 'right, align=right'\n",
    "    else:\n",
    "        ytlab = 'right, align=left'\n",
    "        vtlab = 'right, align=left' if inside == True else 'left, align=left'\n",
    "    # Create ylabel and value label\n",
    "    ylabel = f'\\\\node[{ytlab}, text width=2.0cm] at (axis cs:0,{y}) {{{name}}};'\n",
    "    vlabel = f'\\\\node[{vtlab}] at (axis cs:{x},{y}) {{{vt}}};'\n",
    "    txt.append(bar)\n",
    "    txt.append(ylabel)\n",
    "    txt.append(vlabel)\n",
    "nodes = '\\n'.join(txt)\n",
    "write_txt(text_dir / f'payprod_bars.txt', nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7c716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf37b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02def1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
